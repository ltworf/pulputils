#!/usr/bin/python3
# pulputils
# Copyright (C) 2017  Salvo "LtWorf" Tomaselli
#
# pulputils is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
# author Salvo "LtWorf" Tomaselli <tiposchi@tiscali.it>

import argparse
import hashlib
import itertools
import os
import sys

import pulputils

_ = pulputils._

class Counters:
    def __init__(self):
        self.files = 0
        self.headers = 0
        self.hashes = 0

    def printReport(self):
        print(_('Files scanned:  \t %d\n'
                'Headers read:   \t %d\n'
                'Hashes computed:\t %d\n') % (
                    self.files,
                    self.headers,
                    self.hashes))
counters = Counters()


def findreg(path):
    '''
    yields regular files inside a path
    '''
    path = os.path.realpath(path)
    for dirname, _, files in os.walk(path):
        for i in files:
            item = dirname + '/' + i
            if os.path.islink(item):
                continue
            elif os.path.isfile(item):
                counters.files += 1
                yield item


def group_by_size(paths, minsize: int, skip_manylinks: bool):
    '''
    paths is an iterable containing paths of regular files

    returns a list of lists.
    All the files  in the sublists have the same size.
    '''
    r = {}

    for i in paths:
        stat_st = os.stat(i)
        size = stat_st.st_size
        if size < minsize:
            continue
        if skip_manylinks and stat_st.st_nlink > 1:
            continue
        l = r.get(size, [])
        l.append(i)
        r[size] = l
    return list(r.values())


def find_duplicates(paths):
    '''
    reads files to find the ones that are identical

    generator, returns lists of identical files
    '''
    if len(paths) < 2:
        return []

    # Files that have the same header
    initial_groups = {}

    for i in paths:
        counters.headers += 1
        try:
            with open(i, 'rb') as f:
                key = f.read(1024)
        except PermissionError:
            print(_('Permission denied: %s') % i, file=sys.stderr)
            sys.exit(1)
        l = initial_groups.get(key, [])
        l.append(i)
        initial_groups[key] = l

    same_hash = {}
    for i in initial_groups.values():
        if len(i) < 2:
            continue
        for filename in i:
            counters.hashes += 1
            with open(filename, 'rb') as f:
                h = hashlib.sha512()
                while True:
                    block = f.read(4096)
                    if not len(block):
                        break
                    h.update(block)
                sha = h.digest()
            l = same_hash.get(sha, [])
            l.append(filename)
            same_hash[sha] = l

    for i in same_hash.values():
        if len(i) < 2:
            continue
        yield i


def list_duplicates(duplicate_groups):
    '''
    Prints a lists all duplicate files, splitting them into
    groups.
    '''
    for group in duplicate_groups:
        size = pulputils.human_size(os.stat(group[0]).st_size)
        print(_('>\t%d files. %s each') % (len(group), size))
        print('\n'.join(group))


def calculate_waste(duplicate_groups):
    '''
    Calculate and print how many space is wasted by
    duplicate files
    '''
    r = 0
    for group in duplicate_groups:
        size = os.stat(group[0]).st_size
        r += size * (len(group) - 1)
    print(_('%s wasted.') % pulputils.human_size(r))


def delete_duplicates(duplicate_groups, verbose: bool):
    '''
    Deletes duplicate files, leaving one copy
    '''
    c = 0
    for group in duplicate_groups:
        for duplicate in group[1:]:
            c += 1
            verbose and print(_('Deleting %s') % duplicate)
            os.unlink(duplicate)
    print(_('Deleted %d files') % c)


def join_files(duplicate_groups, verbose: bool, symlink: bool) -> None:
    '''
    Files is a list of identical files

    They will get deleted and replaced by a symlink
    to one of them
    '''
    link_f = os.symlink if symlink else os.link

    for files in duplicate_groups:
        basefile = os.path.realpath(files[0])

        for file in files[1:]:
            file = os.path.realpath(file)

            if symlink:
                common = os.path.commonpath((basefile, file))
                relfile = file[len(common)+1:]
                relbasefile = basefile[len(common)+1:]
                up = itertools.repeat('..', len(relfile.split(os.path.sep)) - 1)
                target = '/'.join(itertools.chain(up,(relbasefile,)))
            else:
                target = basefile
            os.unlink(file)
            verbose and print(_('%s -> %s') % (file, target))
            link_f(target, file)


def main():
    parser = argparse.ArgumentParser(
        description=_('Locates duplicate regular files'),
        formatter_class=argparse.RawTextHelpFormatter
    )
    parser.add_argument(
        'paths',
        metavar='DIRECTORY',
        type=str,
        nargs='+',
        help=_('Directory')
    )
    parser.add_argument('-v', '--verbose', action='store_true')
    parser.add_argument(
        '--version',
        action='version',
        version=pulputils.VERBOSE_VERSION,
        help=_('show program\'s version number and exit')
    )
    parser.add_argument(
        '-c',
        '--calculate',
        action='store_true',
        help=_('calculate the amount of wasted space')
    )
    parser.add_argument(
        '-l',
        '--list',
        action='store_true',
        help=_('list all groups of duplicate files')
    )
    parser.add_argument(
        '-m',
        '--minsize',
        action='store',
        type=int,
        help=_('ignore files smaller than this size. Defaults to 1'),
        default=1,
    )
    parser.add_argument(
        '-D',
        '--delete',
        action='store_true',
        help=_('delete all duplicate files except one')
    )
    parser.add_argument(
        '-L',
        '--link',
        action='store_true',
        help=_('hard link duplicate files together')
    )
    parser.add_argument(
        '-s',
        '--symlink',
        action='store_true',
        help=_('symlink duplicate files together')
    )
    parser.add_argument(
        '-C',
        '--counters',
        action='store_true',
        help=_('prints a report at the end, to know how many operations were performed')
    )
    parser.add_argument(
        '-H',
        '--hardlinks',
        action='store_false',
        help=_('do not skip files with multiple hardlinks')
    )

    args = parser.parse_args()

    # Make sure only 1 action is selected
    if len(list(filter(None, (args.link, args.symlink, args.delete)))) > 1:
        print(_('Only one action can be selected.'), file=sys.stderr)
        sys.exit(2)

    # Locate duplicate files
    args.verbose and print(_('Searching for regular files...'))
    regfiles = itertools.chain(*[findreg(i) for i in args.paths])
    args.verbose and print(_('Grouping files by size...'))
    size_groups = group_by_size(regfiles, args.minsize, args.hardlinks)
    args.verbose and print(_('Grouping identical files...'))
    duplicate_groups = list(itertools.chain(*[find_duplicates(i) for i in size_groups]))

    # Show results
    if args.list:
        list_duplicates(duplicate_groups)
    if args.calculate:
        calculate_waste(duplicate_groups)

    # Take actions
    if args.delete:
        delete_duplicates(duplicate_groups, args.verbose)
    if args.symlink:
        join_files(duplicate_groups, args.verbose, True)
    if args.link:
        join_files(duplicate_groups, args.verbose, False)

    # Show counters
    if args.counters:
        counters.printReport()


if __name__ == '__main__':
    main()
