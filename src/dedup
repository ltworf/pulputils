#!/usr/bin/python3
# pulputils
# Copyright (C) 2017  Salvo "LtWorf" Tomaselli
#
# pulputils is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
# author Salvo "LtWorf" Tomaselli <tiposchi@tiscali.it>

import argparse
import hashlib
import itertools
import os
import sys

import pulputils

_ = pulputils._


def findreg(path):
    '''
    yields regular files inside a path
    '''
    path = os.path.realpath(path)
    for dirname, _, files in os.walk(path):
        for i in files:
            item = dirname + '/' + i
            if os.path.islink(item):
                continue
            elif os.path.isfile(item):
                yield item


def group_by_size(paths):
    '''
    paths is an iterable containing paths of regular files

    returns a list of lists.
    All the files  in the sublists have the same size.
    '''
    r = {}

    for i in paths:
        size = os.stat(i).st_size
        l = r.get(size, [])
        l.append(i)
        r[size] = l
    return list(r.values())


def find_duplicates(paths):
    '''
    reads files to find the ones that are identical

    generator, returns lists of identical files
    '''
    if len(paths) < 2:
        return []

    # Files that have the same header
    initial_groups = {}

    for i in paths:
        with open(i, 'rb') as f:
            key = f.read(64)
        l = initial_groups.get(key, [])
        l.append(i)
        initial_groups[key] = l

    same_hash = {}
    for i in initial_groups.values():
        if len(i) < 2:
            continue
        for filename in i:
            with open(filename, 'rb') as f:
                sha = hashlib.sha512(f.read()).digest()
            l = same_hash.get(sha, [])
            l.append(filename)
            same_hash[sha] = l

    for i in same_hash.values():
        if len(i) < 2:
            continue
        yield i


def list_duplicates(duplicate_groups):
    '''
    Prints a lists all duplicated files, splitting them into
    groups.
    '''
    for group in duplicate_groups:
        size = pulputils.human_size(os.stat(group[0]).st_size)
        print(_('>\t%d files. %s each') % (len(group), size))
        print('\n'.join(group))


def calculate_waste(duplicate_groups):
    '''
    Calculate and print how many space is wasted by
    duplicated files
    '''
    r = 0
    for group in duplicate_groups:
        size = os.stat(group[0]).st_size
        r += size * (len(group) - 1)
    print(_('%s wasted.') % pulputils.human_size(r))


def main():
    parser = argparse.ArgumentParser(
        description=_('Locates duplicate regular files'),
        formatter_class=argparse.RawTextHelpFormatter
    )
    parser.add_argument(
        'paths',
        metavar='DIRECTORY',
        type=str,
        nargs='+',
        help=_('Directory')
    )
    parser.add_argument('-v', '--verbose', action='store_true')
    parser.add_argument(
        '--version',
        action='version',
        version=pulputils.VERBOSE_VERSION,
        help=_('show program\'s version number and exit')
    )
    parser.add_argument(
        '-c',
        '--calculate',
        action='store_true',
        help=_('calculate the amount of wasted space')
    )
    parser.add_argument(
        '-l',
        '--list',
        action='store_true',
        help=_('list all groups of duplicated files')
    )

    args = parser.parse_args()

    args.verbose and print(_('Searching for regular files...'))
    regfiles = itertools.chain(*[findreg(i) for i in args.paths])
    args.verbose and print(_('Grouping files by size...'))
    size_groups = group_by_size(regfiles)
    args.verbose and print(_('Grouping identical files...'))
    duplicate_groups = list(itertools.chain(*[find_duplicates(i) for i in size_groups]))

    if args.list:
        list_duplicates(duplicate_groups)
    if args.calculate:
        calculate_waste(duplicate_groups)


if __name__ == '__main__':
    main()

